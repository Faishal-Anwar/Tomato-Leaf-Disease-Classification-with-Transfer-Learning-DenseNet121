# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17kGCF-9jHyevy7CTSDSPEnTKitSi98fM

# **Image Classification Project: Tomato Leaf Disease Identification**


---

## **Presented by**
Faishal Anwar Hasyim

[LinkedIn](https://www.linkedin.com/in/faishal-anwar-hasyim-1391682a5/) | [Instagram](https://www.instagram.com/faishalah97?igsh=azA0dGFjM3lkd2Jm) | [Github](https://github.com/Faishal-Anwar) |
[Dicoding Profile](https://www.dicoding.com/users/anwarfaishal86/academies)

## **Objective**
This project aims to develop a machine learning model using an image classification approach to accurately identify various types of diseases on tomato leaves and distinguish them from healthy leaves. By leveraging the public kaustubhb999/tomatoleaf dataset, which contains 10 classes of leaf conditions, this model is designed to be an automated diagnostic tool.

[dataset link](https://www.kaggle.com/datasets/kaustubhb999/tomatoleaf) : tomatoleaf dataset

## 1. Import Libraries
"""

import os  # Operating system interfaces
import tensorflow as tf                                    # TensorFlow deep learning framework
import matplotlib.pyplot as plt                            # Plotting library
import matplotlib.image as mpimg                           # Image loading and manipulation library
from PIL import Image
import numpy as np
from tensorflow.keras.models import Sequential, Model      # Sequential and Functional API for building models
from tensorflow.keras.optimizers import Adam               # Adam optimizer for model training
from tensorflow.keras.callbacks import EarlyStopping       # Early stopping callback for model training
from tensorflow.keras.regularizers import l1, l2           # L1 and L2 regularization for model regularization
from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Data augmentation and preprocessing for images
from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D, AveragePooling2D, MaxPooling2D, BatchNormalization
# Various types of layers for building neural networks
from tensorflow.keras.applications import DenseNet121, EfficientNetB4, Xception, VGG16, VGG19   # Pre-trained models for transfer learning

"""# 2. Loading Dataset"""

from google.colab import files
files.upload()  # pilih file kaggle.json (api key) from kaggle

# Download kaggle dataset and unzip the file
!mkdir -p /root/.kaggle
!cp kaggle.json /root/.kaggle/kaggle.json
!chmod 600 /root/.kaggle/kaggle.json
!kaggle datasets download -d kaustubhb999/tomatoleaf
!unzip tomatoleaf.zip

"""# 3. Data preprocessing"""

train_data = tf.keras.utils.image_dataset_from_directory(
    'tomato/train',
    labels='inferred',
    label_mode='categorical',
    image_size=(256, 256),
    batch_size=32)

train_data = train_data.map(lambda x, y: (x / 255.0, y))

val_data = tf.keras.preprocessing.image_dataset_from_directory(
    'tomato/val',
    labels='inferred',
    label_mode='categorical',
    image_size=(256, 256),
    batch_size=32)

val_data = val_data.map(lambda x, y: (x / 255.0, y))

import os
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

# Path folder train
path = "tomato/train"

# Inisialisasi kamus
tomato_image = {}

# Ambil hanya folder kelas (subdirektori)
for class_name in os.listdir(path):
    class_path = os.path.join(path, class_name)
    if os.path.isdir(class_path):
        # Ambil hanya file gambar di dalam folder kelas
        images = [f for f in os.listdir(class_path)
                  if os.path.isfile(os.path.join(class_path, f)) and f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        if images:
            tomato_image[class_name] = images

# Visualisasi 5 gambar acak dari tiap kelas
fig, axs = plt.subplots(len(tomato_image.keys()), 5, figsize=(15, 3 * len(tomato_image)))

for i, class_name in enumerate(tomato_image.keys()):
    images = np.random.choice(tomato_image[class_name], 5, replace=False)
    for j, image_name in enumerate(images):
        img_path = os.path.join(path, class_name, image_name)
        img = Image.open(img_path)
        axs[i, j].imshow(img)
        axs[i, j].set_title(class_name)
        axs[i, j].axis('off')

fig.tight_layout()
plt.show()

"""# 4. Modeling"""

conv_base = DenseNet121(
    weights='imagenet',
    include_top = False,
    input_shape=(256,256,3),
    pooling='avg'
)

conv_base.trainable = False

# Summary of the pretrained model
conv_base.summary()

model = Sequential()
model.add(conv_base)
model.add(BatchNormalization())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.35))
model.add(BatchNormalization())
model.add(Dense(120, activation='relu'))
model.add(Dense(10, activation='softmax'))

model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

# history = model.fit(train_ds,epochs=10,validation_data=validation_ds)
history = model.fit(train_data, epochs=100, validation_data=val_data, callbacks=[EarlyStopping(patience=0)])

"""# 5. Evaluation"""

# Evaluate the model on the validation data
evaluation = model.evaluate(val_data)

# Print the evaluation metrics
print("Validation Loss:", evaluation[0])
print("Validation Accuracy:", evaluation[1])

import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import numpy as np

# Asumsi variabel `history` sudah didapat dari model.fit()
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(len(acc))

# Plot Horizontal: Accuracy
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(epochs, acc, 'r', label='Train Accuracy')
plt.plot(epochs, val_acc, 'b', label='Val Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot Horizontal: Loss
plt.subplot(1, 2, 2)
plt.plot(epochs, loss, 'r', label='Train Loss')
plt.plot(epochs, val_loss, 'b', label='Val Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Ambil label sebenarnya dan prediksi model dari data validasi
y_true = []
y_pred = []

for images, labels in val_data:
    preds = model.predict(images)
    y_true.extend(np.argmax(labels.numpy(), axis=1))
    y_pred.extend(np.argmax(preds, axis=1))

# Ambil class names (jika val_data sudah dimap)
class_names = sorted(os.listdir('tomato/val'))  # Pastikan urutan sesuai label

# Report dalam bentuk dict â†’ DataFrame
report_dict = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)
report_df = pd.DataFrame(report_dict).transpose().round(3)

# Tampilkan tabel
print("ðŸ“Š Classification Report (tabel):")
display(report_df)

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
from google.colab import files
from PIL import Image
import os

# Upload gambar
uploaded = files.upload()
img_path = list(uploaded.keys())[0]

# Buka, ubah ukuran, dan tampilkan gambar
img = Image.open(img_path).convert('RGB')
img = img.resize((256, 256))  # Sesuaikan dengan input_shape modelmu

plt.imshow(img)
plt.axis('off')
plt.title("Gambar yang Di-upload")
plt.show()

# Konversi ke array dan normalisasi
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = img_array / 255.0  # Normalisasi

# Prediksi
prediction = model.predict(img_array)
predicted_class = np.argmax(prediction)

# Ambil nama kelas dari folder validasi
class_names = sorted(os.listdir('tomato/val'))

# Format output
nama_penyakit = class_names[predicted_class].replace("Tomato___", "").replace("_", " ")
print("âœ… penyakit :", nama_penyakit)

# Upload gambar
uploaded = files.upload()
img_path = list(uploaded.keys())[0]

# Buka, ubah ukuran, dan tampilkan gambar
img = Image.open(img_path).convert('RGB')
img = img.resize((256, 256))  # Sesuaikan dengan input_shape modelmu

plt.imshow(img)
plt.axis('off')
plt.title("Gambar yang Di-upload")
plt.show()

# Konversi ke array dan normalisasi
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = img_array / 255.0  # Normalisasi

# Prediksi
prediction = model.predict(img_array)
predicted_class = np.argmax(prediction)

# Ambil nama kelas dari folder validasi
class_names = sorted(os.listdir('tomato/val'))

# Format output
nama_penyakit = class_names[predicted_class].replace("Tomato___", "").replace("_", " ")
print("âœ… penyakit :", nama_penyakit)

# Upload gambar
uploaded = files.upload()
img_path = list(uploaded.keys())[0]

# Buka, ubah ukuran, dan tampilkan gambar
img = Image.open(img_path).convert('RGB')
img = img.resize((256, 256))  # Sesuaikan dengan input_shape modelmu

plt.imshow(img)
plt.axis('off')
plt.title("Gambar yang Di-upload")
plt.show()

# Konversi ke array dan normalisasi
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = img_array / 255.0  # Normalisasi

# Prediksi
prediction = model.predict(img_array)
predicted_class = np.argmax(prediction)

# Ambil nama kelas dari folder validasi
class_names = sorted(os.listdir('tomato/val'))

# Format output
nama_penyakit = class_names[predicted_class].replace("Tomato___", "").replace("_", " ")
print("âœ… penyakit :", nama_penyakit)

"""## 6. Save Model"""

model.save("tomato_model.h5")  # Format HDF5

!pip freeze > requirements.txt

